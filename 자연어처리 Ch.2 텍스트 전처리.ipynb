{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어의 유형\n",
    "\n",
    "1. 굴절어 (ex. 라틴어, 독일어, 러시아어) : 단어의 형태가 변하면서 문법적 기능이 정해짐\n",
    "2. 고립어 (ex. 영어, 중국어) : 어순에 따라 단어의 문법적 기능이 정해짐\n",
    "3. 교착어 (ex. 한국어, 일본어) : 어간에 접사가 붙어 의미와 문법적 기능이 정해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 동사의 변형 사례\n",
    "\n",
    "원형 + (피동 or 높임 or 과거 or 추측 or 전달)\n",
    "\n",
    "피동 (ex +히)\n",
    "높임 (ex +시)\n",
    "과거 (ex +었/았)\n",
    "추측 (ex +겠)\n",
    "전달 (ex +더라)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어는 어순이 그다지 중요하지 않음\n",
    "\n",
    "ex 1. 나는 밥을 먹으러 간다\n",
    "   2. 간다 나는 밥을 먹으러\n",
    "   3. 먹으러 간다 나는 밥을\n",
    "   4. 밥을 먹으러 간다 나는\n",
    "   5. 나는 먹으러 간다 밥을 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어의 주요 특징\n",
    "\n",
    "1. 평서문과 의문문의 내용이 같을 수 있음\n",
    "   예) \"점심 먹었어\" 와 \"점심 먹었어?\"\n",
    "   \n",
    "2. 주어가 자주 생략됨\n",
    "   예) \"점심 먹었어\" 와 \"점심 먹었어?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 음절, 형태소, 어절, 품사\n",
    "\n",
    "1. 음절(syllable) : 하나의 덩어리로 여겨지는 글자 단위. 초성, 중성, 종성으로 이루어져 있음 \n",
    "2. 형태소(morpheme) : 언어에서 의미를 가지는 가장 작은 단위. 형태소를 쪼개면 더 이상 기능이나 의미를 갖지 않음\n",
    "3. 어절 : 한 개 이상의 형태소가 모여 구성된 단위. 띄어쓰기 단위와 거의 일치\n",
    "4. 품사(part-of-speech, pos) : 한국어에서는 해당 단어가 수행하는 역할을 기준으로 체언, 수식언, 관계언, 독립언, 용언의 5언으로 나눔.\n",
    "                               의미에 따라서는 명사, 대명사, 수사, 관형사, 부사, 조사, 감탄사, 동사, 형용사의 9품사로 나눔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 교재 2장 내용\n",
    "\n",
    "1. 토큰화 : 문장의 구성 요소인 토큰 단위로 나누는 것. 단어 토큰화와 문장 토큰화가 있음\n",
    "2. 어간 추출과 표제어 추출\n",
    "3. 불용어(Stopword) : 자주 나오지만 정보 전달력이 약한 단어들을 걸러내는 것\n",
    "4. 정규 표현식(Regular expression) : 불필요한 부분을 분리하는 것\n",
    "5. 원-핫 인코딩(One-hot encoding) : 각 단어에 고유한 단어 벡터를 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화(Tokenization)\n",
    "\n",
    "* 의미있는 단위(단어 또는 문장)로 문장들을 분리하는 것\n",
    "* 단어 토큰화(Word Tokenization)\n",
    "  - 문장을 단어 또는 의미있는 문자열로 구분하는 것\n",
    "  - 구두점(. , ? ! ; 등의 기호)을 포함하는 것이 좋음\n",
    "* 문장 토큰화(Sentence Tokenization)\n",
    "  - 텍스트를 문장별로 나누는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk를 이용한 영어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Dont't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Dont't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dont', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer 단어 토큰화\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Dont't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 문장 토큰화\n",
    "\n",
    "* 한글의 경우 단어 토큰화는 잘 사용되지 않음. 문장을 형태소 분석기에 입력하면 품사로 나뉘어진 단어들을 얻을 수 있음\n",
    "* 문장을 구분하는 경우 KSS(Korean Sentence Splitter)를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk를 이용한 영어 품사 태깅(Part-Of-Speech tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk에서는 Penn Treebank POS Tags를 기준으로 태깅을 수행\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "x = word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# koNLPy를 이용한 형태소(morpheme) 분석\n",
    "\n",
    "* koNLPy는 형태소 분석기를 모은 패키지임 (Okt, Komoran, Hannanum, Kkma, Mecab 등이 지원됨)\n",
    "* 각 분석기에서 다음 함수들이 지원됨\n",
    "  - morphs : 품사를 표시하지 않고 형태소 별로 분리함\n",
    "  - pos : 품사 태깅(part-of-speech tagging)\n",
    "  - nouns : 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# koNLPy를 이용한 분석 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "kkma = Kkma()\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석기 성능\n",
    "\n",
    "Mecab > Twitter > Hannanum > komoran > Kkma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 표제어 추출과 어간 추출\n",
    "\n",
    "* 표제어 추출(Lemmatization) : 영어에서 am, are, is의 표제어는 be이다.\n",
    "  - nltk에서는 WordNetLemmatizer 함수로 진행\n",
    "  \n",
    "* 어간 추출(Stemming) : nltk에서는 PorterStemmer 함수로 기능을 수행\n",
    "* 한글의 경우 표제어와 어간을 추출하는 프로그램은 아직 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어(Stopword)\n",
    "\n",
    "* 사용 빈도와 관계없이 문서의 의미 전달 측면에서는 큰 의미가 없는 단어들을 stopword라고 함\n",
    "* 영어의 경우 nltk에서 불용어 리스트를 제시하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규 표현식(Regular expression)\n",
    "\n",
    "* 정규 표현식은 특정한 패턴을 찾기 위한 방식임\n",
    "* 파이썬에서는 re 모듈에서 함수들이 지원되고 있음\n",
    "* search 함수에 의해 text내에 pattern이 존재하는지 알려줌\n",
    "* 패턴이 존재하지 않으면 응답이 없고, 존재하면 message가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 10), match='at'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.search(\"at\",\"string data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.search(\"at\",\"this is another string\") # 응답이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile 함수의 기능\n",
    "\n",
    "* re.compile() : 정규 표현식의 패턴을 컴파일함. search 할 때 패턴을 지정하지 않게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 10), match='at'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile을 사용하지 않는 경우\n",
    "import re\n",
    "re.search(\"at\",\"string data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(8, 10), match='at'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile을 사용하면 패턴을 저장함\n",
    "import re\n",
    "r1 = re.compile(\"at\")\n",
    "r1.search(\"string data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규 표현식 처리 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']\n",
      "['100', '101', '102']\n",
      "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']\n",
      "['PROF', 'STUD', 'STUD']\n",
      "['John', 'James', 'Mac']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"100 John PROF\n",
    "101 James STUD\n",
    "102 Mac STUD\"\"\"\n",
    "\n",
    "x1 = re.split('\\s+', text)\n",
    "x2 = re.findall('\\d+', text)\n",
    "x3 = re.findall('[A-Z]', text)\n",
    "x4 = re.findall('[A-Z]{4}', text)\n",
    "x5 = re.findall('[A-Z][a-z]+', text)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩\n",
    "\n",
    "* 기계 번역처럼 컴퓨터로 텍스트를 처리하는 프로그램을 만들 때 각 단어에 고유한 정수를 대응시키는 것이 필요함\n",
    "* 텍스트에 단어가 10,000개가 있으면 단어 각각에 1~10,000까지의 숫자를 할당하게 됨\n",
    "* 숫자는 랜덤하게 부여할 수도 있고 빈도수가 높은 순서로 부여하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원-핫 인코딩(One-hot encoding)\n",
    "\n",
    "* 원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 단어별로 한 개의 인덱스만 1, 나머지 인덱스는 0이 되도록 표현하는 방식\n",
    "* 단어의 개수가 많으면 벡터의 차원이 커짐\n",
    "* 이 방법으로는 단어간의 유사성을 표현하지 못함\n",
    "* 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화하는 방식은 워드 임베딩이며, 머신 러닝 기법을 이용하여 벡터 데이터를 구축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 인코딩 사례\n",
    "\n",
    "* 형태소 분석을 통해 문장을 나눈 다음, 발생 순서별로 고유 인덱스를 부여\n",
    "* one_hot_encoding 함수에서 고유 벡터를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n",
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 문장을 분석하여 단어들을 token에 저장\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)\n",
    "\n",
    "word2index = {}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        \n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0] * (len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "one_hot_encoding(\"자연어\", word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
